{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53b12978-583f-44e6-8593-7c132b8adfc1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'BELA'...\n",
      "remote: Enumerating objects: 3811, done.\u001b[K\n",
      "remote: Counting objects: 100% (990/990), done.\u001b[K\n",
      "remote: Compressing objects: 100% (484/484), done.\u001b[K\n",
      "remote: Total 3811 (delta 568), reused 864 (delta 457), pack-reused 2821\u001b[K\n",
      "Receiving objects: 100% (3811/3811), 16.39 MiB | 3.29 MiB/s, done.\n",
      "Resolving deltas: 100% (2458/2458), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/facebookresearch/BELA.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b2fbd37-bca4-4555-91ef-f2998393a086",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/el/BELA\n"
     ]
    }
   ],
   "source": [
    "cd BELA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189940cf-850d-47ec-a248-474c2d8b94ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-06-27 23:23:12--  http://dl.fbaipublicfiles.com/bela/models/model_aida.ckpt\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 65.9.181.94, 65.9.181.7, 65.9.181.27, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|65.9.181.94|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2243429223 (2.1G) [binary/octet-stream]\n",
      "Saving to: ‘model_aida.ckpt’\n",
      "\n",
      "model_aida.ckpt     100%[===================>]   2.09G  21.1MB/s    in 2m 3s   \n",
      "\n",
      "2023-06-27 23:25:16 (17.4 MB/s) - ‘model_aida.ckpt’ saved [2243429223/2243429223]\n",
      "\n",
      "--2023-06-27 23:25:17--  http://dl.fbaipublicfiles.com/bela/models/model_mewsli.ckpt\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 65.9.181.94, 65.9.181.33, 65.9.181.27, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|65.9.181.94|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2243429835 (2.1G) [binary/octet-stream]\n",
      "Saving to: ‘model_mewsli.ckpt’\n",
      "\n",
      "model_mewsli.ckpt   100%[===================>]   2.09G  23.3MB/s    in 1m 45s  \n",
      "\n",
      "2023-06-27 23:27:02 (20.4 MB/s) - ‘model_mewsli.ckpt’ saved [2243429835/2243429835]\n",
      "\n",
      "--2023-06-27 23:27:02--  http://dl.fbaipublicfiles.com/bela/models/model_wiki.ckpt\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 65.9.181.7, 65.9.181.33, 65.9.181.27, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|65.9.181.7|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2243434498 (2.1G) [binary/octet-stream]\n",
      "Saving to: ‘model_wiki.ckpt’\n",
      "\n",
      "model_wiki.ckpt     100%[===================>]   2.09G  21.9MB/s    in 2m 10s  \n",
      "\n",
      "2023-06-27 23:29:13 (16.5 MB/s) - ‘model_wiki.ckpt’ saved [2243434498/2243434498]\n",
      "\n",
      "--2023-06-27 23:29:13--  http://dl.fbaipublicfiles.com/bela/models/model_e2e.ckpt\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 65.9.181.7, 65.9.181.27, 65.9.181.94, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|65.9.181.7|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2243429835 (2.1G) [binary/octet-stream]\n",
      "Saving to: ‘model_e2e.ckpt’\n",
      "\n",
      "model_e2e.ckpt      100%[===================>]   2.09G  17.2MB/s    in 3m 46s  \n",
      "\n",
      "2023-06-27 23:33:01 (9.46 MB/s) - ‘model_e2e.ckpt’ saved [2243429835/2243429835]\n",
      "\n",
      "--2023-06-27 23:33:01--  http://dl.fbaipublicfiles.com/bela/embeddings/index.txt\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 65.9.181.7, 65.9.181.33, 65.9.181.94, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|65.9.181.7|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 158216234 (151M) [binary/octet-stream]\n",
      "Saving to: ‘index.txt’\n",
      "\n",
      "index.txt           100%[===================>] 150.89M  22.6MB/s    in 7.8s    \n",
      "\n",
      "2023-06-27 23:33:09 (19.3 MB/s) - ‘index.txt’ saved [158216234/158216234]\n",
      "\n",
      "--2023-06-27 23:33:09--  http://dl.fbaipublicfiles.com/bela/embeddings/embeddings.pt\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 65.9.181.33, 65.9.181.94, 65.9.181.27, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|65.9.181.33|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 19765027987 (18G) [binary/octet-stream]\n",
      "Saving to: ‘embeddings.pt’\n",
      "\n",
      "embeddings.pt        56%[==========>         ]  10.33G  6.90MB/s    eta 15m 18s"
     ]
    }
   ],
   "source": [
    "!./download_models.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f0f00c6-92d7-402b-87c0-22c40c457b24",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-gpu (from -r requirements.txt (line 1))\n",
      "  Downloading faiss_gpu-1.7.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting fairscale (from -r requirements.txt (line 2))\n",
      "  Downloading fairscale-0.4.13.tar.gz (266 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.3/266.3 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting hydra-core (from -r requirements.txt (line 3))\n",
      "  Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting hydra-submitit-launcher (from -r requirements.txt (line 4))\n",
      "  Downloading hydra_submitit_launcher-1.2.0-py3-none-any.whl (5.2 kB)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 5)) (6.0)\n",
      "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 6)) (1.9.1)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 7)) (4.30.0.dev0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 8)) (4.65.0)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 9)) (0.1.97)\n",
      "Collecting h5py (from -r requirements.txt (line 10))\n",
      "  Downloading h5py-3.9.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting protobuf==3.20 (from -r requirements.txt (line 11))\n",
      "  Downloading protobuf-3.20.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: ujson in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 12)) (5.8.0)\n",
      "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.8/dist-packages (from fairscale->-r requirements.txt (line 2)) (1.13.1)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /usr/local/lib/python3.8/dist-packages (from fairscale->-r requirements.txt (line 2)) (1.24.2)\n",
      "Collecting omegaconf<2.4,>=2.2 (from hydra-core->-r requirements.txt (line 3))\n",
      "  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting antlr4-python3-runtime==4.9.* (from hydra-core->-r requirements.txt (line 3))\n",
      "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from hydra-core->-r requirements.txt (line 3)) (23.0)\n",
      "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.8/dist-packages (from hydra-core->-r requirements.txt (line 3)) (5.12.0)\n",
      "Collecting submitit>=1.3.3 (from hydra-submitit-launcher->-r requirements.txt (line 4))\n",
      "  Downloading submitit-1.4.5-py3-none-any.whl (73 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.1/73.1 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning->-r requirements.txt (line 6)) (2023.3.0)\n",
      "Requirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning->-r requirements.txt (line 6)) (0.11.4)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning->-r requirements.txt (line 6)) (4.5.0)\n",
      "Requirement already satisfied: lightning-utilities>=0.6.0.post0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning->-r requirements.txt (line 6)) (0.8.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers->-r requirements.txt (line 7)) (3.10.7)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.8/dist-packages (from transformers->-r requirements.txt (line 7)) (0.14.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers->-r requirements.txt (line 7)) (2023.3.23)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers->-r requirements.txt (line 7)) (2.28.2)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers->-r requirements.txt (line 7)) (0.13.2)\n",
      "Requirement already satisfied: safetensors>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from transformers->-r requirements.txt (line 7)) (0.3.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.8/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning->-r requirements.txt (line 6)) (3.8.4)\n",
      "Collecting cloudpickle>=1.2.1 (from submitit>=1.3.3->hydra-submitit-launcher->-r requirements.txt (line 4))\n",
      "  Downloading cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.8/dist-packages (from torch>=1.8.0->fairscale->-r requirements.txt (line 2)) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.8/dist-packages (from torch>=1.8.0->fairscale->-r requirements.txt (line 2)) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.8/dist-packages (from torch>=1.8.0->fairscale->-r requirements.txt (line 2)) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.8/dist-packages (from torch>=1.8.0->fairscale->-r requirements.txt (line 2)) (11.7.99)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.8.0->fairscale->-r requirements.txt (line 2)) (45.2.0)\n",
      "Requirement already satisfied: wheel in /usr/lib/python3/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.8.0->fairscale->-r requirements.txt (line 2)) (0.34.2)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from importlib-resources->hydra-core->-r requirements.txt (line 3)) (3.15.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers->-r requirements.txt (line 7)) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers->-r requirements.txt (line 7)) (2.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers->-r requirements.txt (line 7)) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers->-r requirements.txt (line 7)) (2019.11.28)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning->-r requirements.txt (line 6)) (22.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning->-r requirements.txt (line 6)) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning->-r requirements.txt (line 6)) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning->-r requirements.txt (line 6)) (1.8.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning->-r requirements.txt (line 6)) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning->-r requirements.txt (line 6)) (1.3.1)\n",
      "Building wheels for collected packages: fairscale, antlr4-python3-runtime\n",
      "  Building wheel for fairscale (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fairscale: filename=fairscale-0.4.13-py3-none-any.whl size=332112 sha256=adf4c98e6963477734e7c346b1507556967560a73aba4e93b64c775bb264b9f3\n",
      "  Stored in directory: /root/.cache/pip/wheels/b8/02/9b/dc7d4ff5145afdd28f456dae6605a46619af0370eca30d8d7e\n",
      "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144573 sha256=de33850cc7bd9fd50fe41bde998d98cf566bd9f0b869437e36c184820b31b9a4\n",
      "  Stored in directory: /root/.cache/pip/wheels/b1/a3/c2/6df046c09459b73cc9bb6c4401b0be6c47048baf9a1617c485\n",
      "Successfully built fairscale antlr4-python3-runtime\n",
      "Installing collected packages: faiss-gpu, antlr4-python3-runtime, protobuf, omegaconf, h5py, cloudpickle, submitit, hydra-core, hydra-submitit-launcher, fairscale\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.3\n",
      "    Uninstalling protobuf-3.20.3:\n",
      "      Successfully uninstalled protobuf-3.20.3\n",
      "Successfully installed antlr4-python3-runtime-4.9.3 cloudpickle-2.2.1 fairscale-0.4.13 faiss-gpu-1.7.2 h5py-3.9.0 hydra-core-1.3.2 hydra-submitit-launcher-1.2.0 omegaconf-2.3.0 protobuf-3.20.0 submitit-1.4.5\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26071e32-ddc6-4b33-b3fb-010a9f87d7f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///workspace/el/BELA\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hInstalling collected packages: bela\n",
      "  Running setup.py develop for bela\n",
      "Successfully installed bela-0.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb8ed50e-675f-4c34-8f49-292776a30c28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "885c5baf-bb5b-48b8-b960-b95b76b84149",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from bela.transforms.spm_transform import SPMTransform\n",
    "\n",
    "from bela.evaluation.model_eval import ModelEval, load_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8a0c560-e8b6-4ce7-8273-9ad1726ded59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embeddings_path = \"./models/embeddings.pt\"\n",
    "ent_catalogue_idx_path = \"./models/index.txt\"\n",
    "checkpoint_path = \"./models/model_wiki.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b121c4c-ed14-4d8d-95b4-a7f65840b8ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "md_threshold = 0.2\n",
    "el_threshold = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "149a2d43-08c6-4782-9549-2f32e5d785f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " 'checkpoint_path',\n",
       " 'compute_scores',\n",
       " 'create_gpu_index',\n",
       " 'device',\n",
       " 'embeddings',\n",
       " 'ent_idx',\n",
       " 'faiss_index',\n",
       " 'get_disambiguation_predictions',\n",
       " 'get_predictions',\n",
       " 'lookup',\n",
       " 'process_batch',\n",
       " 'process_disambiguation_batch',\n",
       " 'task',\n",
       " 'transform']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(model_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a243912-efde-4bf5-92fa-fea8e2e5bc14",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from checkpoint ./models/model_wiki.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'joint_el_mel_new': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information\n",
      "  warnings.warn(msg, UserWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/hydra/core/default_element.py:124: UserWarning: In 'task/model/xlmr_large': Usage of deprecated keyword in package header '# @package _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more information\n",
      "  deprecation_warning(\n",
      "/usr/local/lib/python3.8/dist-packages/hydra/core/default_element.py:124: UserWarning: In 'task/transform/joint_el_xlmr_raw_transform_large': Usage of deprecated keyword in package header '# @package _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more information\n",
      "  deprecation_warning(\n",
      "Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaModel: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# joint_el_mel_new xlm-roberta-large\n",
    "print(f\"Loading model from checkpoint {checkpoint_path}\")\n",
    "model_eval = ModelEval(checkpoint_path, config_name=\"joint_el_mel_new\",embeddings_path=embeddings_path,ent_catalogue_idx_path=ent_catalogue_idx_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c5840e-7552-4e7e-bb22-85bfae59b4f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_eval.task.md_threshold = md_threshold\n",
    "model_eval.task.el_threshold = el_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b819bf33-e99f-4e6a-9517-ea685d0a42e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'offsets': [0, 50, 60, 65],\n",
       "  'lengths': [9, 8, 3, 7],\n",
       "  'entities': ['Q869', 'Q881', 'Q819', 'Q424'],\n",
       "  'md_scores': [0.20076893270015717,\n",
       "   0.7526548504829407,\n",
       "   0.6310780644416809,\n",
       "   0.8615027070045471],\n",
       "  'el_scores': [0.4858655035495758,\n",
       "   0.9267557859420776,\n",
       "   0.893130898475647,\n",
       "   0.9612433314323425]}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_eval.process_batch([\"ประเทศไทย เป็นประเทศที่อยู่ในโลก เป็นประเทศคู่แข่งเวียดนาม, ลาว, กัมพูชา\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "376cde10-ea6d-4a2e-8344-e0a4e29e78ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'offsets': [54],\n",
       "  'lengths': [12],\n",
       "  'entities': ['Q1142913'],\n",
       "  'md_scores': [0.5890175700187683],\n",
       "  'el_scores': [0.9352976679801941]}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_eval.process_batch([\"นายกประยุทธ์ประกาศจัดการเลือกตั้ง\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e6ad33a5-0e02-4ada-9d05-3afdaa14a9d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'offsets': [54],\n",
       "  'lengths': [12],\n",
       "  'entities': ['Q1142913'],\n",
       "  'md_scores': [0.5890175700187683],\n",
       "  'el_scores': [0.9352976679801941]}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_eval.process_batch([\"นายกประยุทธ์ประกาศจัดการเลือกตั้งและพร้อมลงสนามแข่งกับพรรคเพื่อไทย\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "346d80d8-811f-4073-89d7-dac2fb86cc47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " 'checkpoint_path',\n",
       " 'compute_scores',\n",
       " 'create_gpu_index',\n",
       " 'device',\n",
       " 'embeddings',\n",
       " 'ent_idx',\n",
       " 'faiss_index',\n",
       " 'get_disambiguation_predictions',\n",
       " 'get_predictions',\n",
       " 'lookup',\n",
       " 'process_batch',\n",
       " 'process_disambiguation_batch',\n",
       " 'task',\n",
       " 'transform']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(model_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4cd85588-2737-48ee-a94e-4fdf10156969",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JointELXlmrRawTextTransform(\n",
       "  (_collate): JointELCollate()\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_eval.transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a05a8c3-5473-4569-b1c8-ce960ef94a1c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JointELTask(\n",
       "  (disambiguation_loss): CrossEntropyLoss()\n",
       "  (md_loss): BCEWithLogitsLoss()\n",
       "  (el_loss): BCEWithLogitsLoss()\n",
       "  (saliency_loss): BCEWithLogitsLoss()\n",
       "  (encoder): HFEncoder(\n",
       "    (transformer): XLMRobertaModel(\n",
       "      (embeddings): XLMRobertaEmbeddings(\n",
       "        (word_embeddings): Embedding(250002, 1024, padding_idx=1)\n",
       "        (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "        (token_type_embeddings): Embedding(1, 1024)\n",
       "        (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): XLMRobertaEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0): XLMRobertaLayer(\n",
       "            (attention): XLMRobertaAttention(\n",
       "              (self): XLMRobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): XLMRobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): XLMRobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): XLMRobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): XLMRobertaLayer(\n",
       "            (attention): XLMRobertaAttention(\n",
       "              (self): XLMRobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): XLMRobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): XLMRobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): XLMRobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): XLMRobertaLayer(\n",
       "            (attention): XLMRobertaAttention(\n",
       "              (self): XLMRobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): XLMRobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): XLMRobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): XLMRobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): XLMRobertaLayer(\n",
       "            (attention): XLMRobertaAttention(\n",
       "              (self): XLMRobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): XLMRobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): XLMRobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): XLMRobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (4): XLMRobertaLayer(\n",
       "            (attention): XLMRobertaAttention(\n",
       "              (self): XLMRobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): XLMRobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): XLMRobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): XLMRobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (5): XLMRobertaLayer(\n",
       "            (attention): XLMRobertaAttention(\n",
       "              (self): XLMRobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): XLMRobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): XLMRobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): XLMRobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (6): XLMRobertaLayer(\n",
       "            (attention): XLMRobertaAttention(\n",
       "              (self): XLMRobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): XLMRobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): XLMRobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): XLMRobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (7): XLMRobertaLayer(\n",
       "            (attention): XLMRobertaAttention(\n",
       "              (self): XLMRobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): XLMRobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): XLMRobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): XLMRobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (8): XLMRobertaLayer(\n",
       "            (attention): XLMRobertaAttention(\n",
       "              (self): XLMRobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): XLMRobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): XLMRobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): XLMRobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (9): XLMRobertaLayer(\n",
       "            (attention): XLMRobertaAttention(\n",
       "              (self): XLMRobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): XLMRobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): XLMRobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): XLMRobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (10): XLMRobertaLayer(\n",
       "            (attention): XLMRobertaAttention(\n",
       "              (self): XLMRobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): XLMRobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): XLMRobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): XLMRobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (11): XLMRobertaLayer(\n",
       "            (attention): XLMRobertaAttention(\n",
       "              (self): XLMRobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): XLMRobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): XLMRobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): XLMRobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (12): XLMRobertaLayer(\n",
       "            (attention): XLMRobertaAttention(\n",
       "              (self): XLMRobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): XLMRobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): XLMRobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): XLMRobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (13): XLMRobertaLayer(\n",
       "            (attention): XLMRobertaAttention(\n",
       "              (self): XLMRobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): XLMRobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): XLMRobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): XLMRobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (14): XLMRobertaLayer(\n",
       "            (attention): XLMRobertaAttention(\n",
       "              (self): XLMRobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): XLMRobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): XLMRobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): XLMRobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (15): XLMRobertaLayer(\n",
       "            (attention): XLMRobertaAttention(\n",
       "              (self): XLMRobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): XLMRobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): XLMRobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): XLMRobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (16): XLMRobertaLayer(\n",
       "            (attention): XLMRobertaAttention(\n",
       "              (self): XLMRobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): XLMRobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): XLMRobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): XLMRobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (17): XLMRobertaLayer(\n",
       "            (attention): XLMRobertaAttention(\n",
       "              (self): XLMRobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): XLMRobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): XLMRobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): XLMRobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (18): XLMRobertaLayer(\n",
       "            (attention): XLMRobertaAttention(\n",
       "              (self): XLMRobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): XLMRobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): XLMRobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): XLMRobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (19): XLMRobertaLayer(\n",
       "            (attention): XLMRobertaAttention(\n",
       "              (self): XLMRobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): XLMRobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): XLMRobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): XLMRobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (20): XLMRobertaLayer(\n",
       "            (attention): XLMRobertaAttention(\n",
       "              (self): XLMRobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): XLMRobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): XLMRobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): XLMRobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (21): XLMRobertaLayer(\n",
       "            (attention): XLMRobertaAttention(\n",
       "              (self): XLMRobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): XLMRobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): XLMRobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): XLMRobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (22): XLMRobertaLayer(\n",
       "            (attention): XLMRobertaAttention(\n",
       "              (self): XLMRobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): XLMRobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): XLMRobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): XLMRobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (23): XLMRobertaLayer(\n",
       "            (attention): XLMRobertaAttention(\n",
       "              (self): XLMRobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): XLMRobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): XLMRobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): XLMRobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): XLMRobertaPooler(\n",
       "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (project_encoder_op): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=300, bias=True)\n",
       "    (1): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (span_encoder): SpanEncoder(\n",
       "    (mention_mlp): Linear(in_features=600, out_features=300, bias=True)\n",
       "  )\n",
       "  (mention_encoder): MentionScoresHead(\n",
       "    (bound_classifier): Linear(in_features=300, out_features=3, bias=True)\n",
       "  )\n",
       "  (el_encoder): ClassificationHead(\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=1202, out_features=300, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.2, inplace=False)\n",
       "      (3): Linear(in_features=300, out_features=300, bias=True)\n",
       "      (4): ReLU()\n",
       "      (5): Dropout(p=0.2, inplace=False)\n",
       "      (6): Linear(in_features=300, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_eval.task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fcf9b6-36be-4f5a-9f4a-42e873f5468f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
